
import os
import subprocess
import boto3
import faiss
import time
import random

# ================= Configuration =================
BUCKET_NAME = "my-judgments-bucket01"
LOG_KEY = "ingestion_log.txt"
LOCAL_LOG = "ingestion_log.txt"
FAISS_KEY = "legal_index.faiss"
LOCAL_FAISS = "legal_index.faiss"
BATCH_SIZE = 50  # number of PDFs per run

# Additional files generated by rag1.py
GENERATED_FILES = ["doc_vecs.npy", "meta.json", "legal_index.faiss"]

# ================= Initialize S3 =================
s3 = boto3.client(
    "s3",
    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    region_name=os.getenv("AWS_DEFAULT_REGION", "ap-south-1")
)

# ================= Log Handling =================
def reset_log():
    """Delete old log from S3 and create a new empty local log."""
    try:
        s3.delete_object(Bucket=BUCKET_NAME, Key=LOG_KEY)
        print("‚úÖ Old S3 log deleted.")
    except s3.exceptions.NoSuchKey:
        print("‚ö†Ô∏è No previous log found in S3.")
    open(LOCAL_LOG, "w").close()
    s3.upload_file(LOCAL_LOG, BUCKET_NAME, LOG_KEY)
    print("‚úÖ New log created in S3.")

def load_log():
    """Load already ingested PDF keys from S3 log."""
    try:
        s3.download_file(BUCKET_NAME, LOG_KEY, LOCAL_LOG)
    except:
        open(LOCAL_LOG, "w").close()
    with open(LOCAL_LOG, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f)

def append_log(pdf_key):
    """Append a processed PDF key to log and sync to S3."""
    with open(LOCAL_LOG, "a", encoding="utf-8") as f:
        f.write(pdf_key + "\n")
    s3.upload_file(LOCAL_LOG, BUCKET_NAME, LOG_KEY)
    print(f"‚úÖ Logged: {pdf_key}")

# ================= List PDFs in S3 =================
def list_pdfs():
    """Return all PDF keys in the S3 bucket."""
    pdf_keys = []
    continuation_token = None
    while True:
        if continuation_token:
            resp = s3.list_objects_v2(Bucket=BUCKET_NAME, ContinuationToken=continuation_token)
        else:
            resp = s3.list_objects_v2(Bucket=BUCKET_NAME)
        pdf_keys.extend([obj["Key"] for obj in resp.get("Contents", []) if obj["Key"].lower().endswith(".pdf")])
        if resp.get("IsTruncated"):
            continuation_token = resp["NextContinuationToken"]
        else:
            break
    return pdf_keys

# ================= FAISS and Files Handling =================
def download_generated_files():
    """Download FAISS index, doc_vecs, and meta if exist in S3."""
    for f in GENERATED_FILES:
        try:
            s3.download_file(BUCKET_NAME, f, f)
            print(f"‚úÖ Downloaded {f} from S3")
        except:
            print(f"‚ö†Ô∏è No existing {f} found; will create new")

def upload_generated_files():
    """Upload all generated files to S3."""
    for f in GENERATED_FILES:
        if os.path.exists(f):
            s3.upload_file(f, BUCKET_NAME, f)
            print(f"‚úÖ Uploaded {f} to S3")
        else:
            print(f"‚ö†Ô∏è File not found locally: {f}")

def load_faiss_index(dim=1536):
    """Load FAISS index or create new."""
    if os.path.exists(LOCAL_FAISS):
        index = faiss.read_index(LOCAL_FAISS)
        print("‚úÖ FAISS index loaded locally.")
    else:
        index = faiss.IndexFlatL2(dim)
        print("‚ö†Ô∏è No FAISS index found. Created new index.")
    return index

def save_faiss_index(index):
    """Save FAISS index locally and upload to S3."""
    faiss.write_index(index, LOCAL_FAISS)
    s3.upload_file(LOCAL_FAISS, BUCKET_NAME, FAISS_KEY)
    print("‚úÖ FAISS index uploaded to S3.")

# ================= Safe Subprocess Runner =================
def run_with_retries(cmd, retries=3, delay=5):
    """Run a subprocess with retries for transient errors."""
    for attempt in range(1, retries + 1):
        try:
            subprocess.run(cmd, check=True)
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Subprocess failed on attempt {attempt}: {e}")
            if attempt < retries:
                wait = delay * attempt + random.uniform(0, 2)
                print(f"‚è≥ Retrying in {wait:.1f}s...")
                time.sleep(wait)
            else:
                print("üö® Max retries reached. Skipping this file.")
                return False

# ================= Main Ingestion =================
def ingest_pdfs_s3(batch_size=BATCH_SIZE):
    processed_files = load_log()
    all_pdfs = [k for k in list_pdfs() if k not in processed_files]

    if not all_pdfs:
        print("‚úÖ All PDFs already processed.")
        return

    batch = all_pdfs[:batch_size]
    print(f"üìÇ Found {len(all_pdfs)} unprocessed PDFs in bucket")
    print(f"üöÄ Starting batch of {len(batch)} PDFs...")

    download_generated_files()
    index = load_faiss_index()

    processed_count = 0

    for key in batch:
        local_file = key.split("/")[-1]
        s3.download_file(BUCKET_NAME, key, local_file)

        cmd = ["python3", "rag1.py", "--ingest", local_file, "--append"]
        print("Running:", " ".join(cmd))

        success = run_with_retries(cmd)

        if success:
            append_log(key)
            # ‚úÖ Delete from S3 after successful processing
            s3.delete_object(Bucket=BUCKET_NAME, Key=key)
            print(f"üóëÔ∏è Deleted {key} from S3.")
            processed_count += 1

        # ‚úÖ Always clean up local file
        if os.path.exists(local_file):
            os.remove(local_file)
            print(f"üßπ Deleted local file {local_file}")

    save_faiss_index(index)
    upload_generated_files()

    print(f"‚úÖ Batch complete: {processed_count}/{len(batch)} PDFs processed")
    print(f"üìä Total processed so far (including previous runs): {len(processed_files) + processed_count}")

# ================= Entry Point =================
if __name__ == "__main__":
    # reset_log()  # Uncomment to start from scratch
    ingest_pdfs_s3()

