import os
import subprocess
import boto3
import faiss

# ================= Configuration =================
BUCKET_NAME = "my-judgments-bucket01"
LOG_KEY = "ingestion_log.txt"
LOCAL_LOG = "ingestion_log.txt"
FAISS_KEY = "legal_index.faiss"
LOCAL_FAISS = "legal_index.faiss"
BATCH_SIZE = 50  # number of PDFs per run

# Additional files generated by rag1.py
GENERATED_FILES = ["doc_vecs.npy", "meta.json", "legal_index.faiss"]

# ================= Initialize S3 =================
s3 = boto3.client(
    "s3",
    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    region_name=os.getenv("AWS_DEFAULT_REGION", "ap-south-1")
)

# ================= Log Handling =================
def reset_log():
    """Delete old log from S3 and create a new empty local log."""
    try:
        s3.delete_object(Bucket=BUCKET_NAME, Key=LOG_KEY)
        print("✅ Old S3 log deleted.")
    except s3.exceptions.NoSuchKey:
        print("⚠️ No previous log found in S3.")
    open(LOCAL_LOG, "w").close()
    s3.upload_file(LOCAL_LOG, BUCKET_NAME, LOG_KEY)
    print("✅ New log created in S3.")

def load_log():
    """Load already ingested PDF keys from S3 log."""
    try:
        s3.download_file(BUCKET_NAME, LOG_KEY, LOCAL_LOG)
    except:
        open(LOCAL_LOG, "w").close()
    with open(LOCAL_LOG, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f)

def append_log(pdf_key):
    """Append a processed PDF key to log and sync to S3."""
    with open(LOCAL_LOG, "a", encoding="utf-8") as f:
        f.write(pdf_key + "\n")
    s3.upload_file(LOCAL_LOG, BUCKET_NAME, LOG_KEY)
    print(f"✅ Logged: {pdf_key}")

# ================= List PDFs in S3 =================
def list_pdfs():
    """Return all PDF keys in the S3 bucket."""
    pdf_keys = []
    continuation_token = None
    while True:
        if continuation_token:
            resp = s3.list_objects_v2(Bucket=BUCKET_NAME, ContinuationToken=continuation_token)
        else:
            resp = s3.list_objects_v2(Bucket=BUCKET_NAME)
        pdf_keys.extend([obj["Key"] for obj in resp.get("Contents", []) if obj["Key"].lower().endswith(".pdf")])
        if resp.get("IsTruncated"):
            continuation_token = resp["NextContinuationToken"]
        else:
            break
    return pdf_keys

# ================= FAISS and Files Handling =================
def download_generated_files():
    """Download FAISS index, doc_vecs, and meta if exist in S3."""
    for f in GENERATED_FILES:
        try:
            s3.download_file(BUCKET_NAME, f, f)
            print(f"✅ Downloaded {f} from S3")
        except:
            print(f"⚠️ No existing {f} found; will create new")

def upload_generated_files():
    """Upload all generated files to S3."""
    for f in GENERATED_FILES:
        if os.path.exists(f):
            s3.upload_file(f, BUCKET_NAME, f)
            print(f"✅ Uploaded {f} to S3")
        else:
            print(f"⚠️ File not found locally: {f}")

def load_faiss_index(dim=1536):
    """Load FAISS index or create new."""
    if os.path.exists(LOCAL_FAISS):
        index = faiss.read_index(LOCAL_FAISS)
        print("✅ FAISS index loaded locally.")
    else:
        index = faiss.IndexFlatL2(dim)
        print("⚠️ No FAISS index found. Created new index.")
    return index

def save_faiss_index(index):
    """Save FAISS index locally and upload to S3."""
    faiss.write_index(index, LOCAL_FAISS)
    s3.upload_file(LOCAL_FAISS, BUCKET_NAME, FAISS_KEY)
    print("✅ FAISS index uploaded to S3.")

# ================= Main Ingestion =================
def ingest_pdfs_s3(batch_size=BATCH_SIZE):
    processed_files = load_log()
    all_pdfs = [k for k in list_pdfs() if k not in processed_files]

    if not all_pdfs:
        print("✅ All PDFs already processed.")
        return

    batch = all_pdfs[:batch_size]
    print(f"Processing batch of {len(batch)} PDFs...")

    download_generated_files()
    index = load_faiss_index()

    for key in batch:
        local_file = key.split("/")[-1]
        s3.download_file(BUCKET_NAME, key, local_file)

        cmd = ["python3", "rag1.py", "--ingest", local_file, "--append"]
        print("Running:", " ".join(cmd))

        try:
            subprocess.run(cmd, check=True)
            append_log(key)
            # Optionally: update FAISS index if rag1.py returns embedding
            # embedding = get_embedding(local_file)
            # index.add(embedding)
        except subprocess.CalledProcessError as e:
            print(f"❌ Error ingesting {key}: {e}")
            break

        try:
            os.remove(local_file)
        except Exception as e:
            print(f"⚠️ Could not delete {local_file}: {e}")

    save_faiss_index(index)
    upload_generated_files()
    print("✅ Finished batch. Remaining PDFs will be processed in next run.")

# ================= Entry Point =================
if __name__ == "__main__":
    # Only reset log if you want a fresh start
    # reset_log()  # Uncomment to start from scratch
    ingest_pdfs_s3()
